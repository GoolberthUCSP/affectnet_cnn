{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importación de librerías necesarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
    "from sklearn.model_selection import StratifiedShuffleSplit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definición del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AffectNetCNN(nn.Module):\n",
    "    def __init__(self, num_classes=2, lr=0.001):\n",
    "        super(AffectNetCNN, self).__init__()\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.conv1_layer = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        self.conv2_layer = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        self.fc_layer = nn.Sequential(\n",
    "            nn.Linear(256 * 14 * 14, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "        self.optimizer = optim.AdamW(self.parameters(), lr=lr)\n",
    "        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            self.optimizer, mode='min', factor=0.5, patience=2\n",
    "        )\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.to(self.device)\n",
    "        x = self.conv1_layer(x)\n",
    "        x = self.conv2_layer(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc_layer(x)\n",
    "        return x\n",
    "\n",
    "    def _loss_(self, x, target, class_weights=None):\n",
    "        x, target = x.to(self.device), target.to(self.device)\n",
    "        if class_weights is not None:\n",
    "            class_weights = class_weights.to(self.device)\n",
    "            loss_fn = nn.CrossEntropyLoss(weight=class_weights)\n",
    "        else:\n",
    "            loss_fn = nn.CrossEntropyLoss()\n",
    "        return loss_fn(x, target)\n",
    "\n",
    "    def train_(self, data_loader, epochs=10, class_weights=None):\n",
    "        for epoch in range(epochs):\n",
    "            total_loss = 0\n",
    "            for x, target in data_loader:\n",
    "                x, target = x.to(self.device), target.to(self.device)\n",
    "                self.optimizer.zero_grad()\n",
    "                loss = self._loss_(self.forward(x), target, class_weights)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                total_loss += loss.item()\n",
    "            self.scheduler.step(total_loss / len(data_loader))\n",
    "            print(f'Epoch {epoch+1}/{epochs}, Loss: {total_loss / len(data_loader)}')\n",
    "\n",
    "    def test_(self, data_loader):\n",
    "        self.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for x, target in data_loader:\n",
    "                x, target = x.to(self.device), target.to(self.device)\n",
    "                output = self.forward(x)\n",
    "                _, predicted = torch.max(output, 1)\n",
    "                total += target.size(0)\n",
    "                correct += (predicted == target).sum().item()\n",
    "        return correct / total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definición del transformador de las imágenes del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224), interpolation=transforms.InterpolationMode.LANCZOS),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Carga del conjunto de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset classes: ['anger', 'contempt', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise']\n",
      "Class counts: Counter({5: 1880, 4: 1862, 7: 1851, 3: 1839, 1: 1833, 0: 1822, 6: 1821, 2: 1740})\n",
      "Weights per class: tensor([1.0049, 0.9989, 1.0523, 0.9956, 0.9834, 0.9739, 1.0055, 0.9892])\n"
     ]
    }
   ],
   "source": [
    "path = os.getcwd()\n",
    "dataset = datasets.ImageFolder(os.path.join(path, 'AffectNet'), transform=transform)\n",
    "num_classes = len(dataset.classes)\n",
    "print('Dataset classes:', dataset.classes)\n",
    "\n",
    "from collections import Counter\n",
    "class_counts = Counter(dataset.targets)\n",
    "total_samples = sum(class_counts.values())\n",
    "num_classes = len(class_counts)\n",
    "class_weights = [total_samples / (num_classes * count) for count in class_counts.values()]\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float32)\n",
    "\n",
    "print(\"Class counts:\", class_counts)\n",
    "print(\"Weights per class:\", class_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Obtención de las etiquetas para la estratificación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = dataset.targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creación de índices de train y test de manera estratificada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 0.3 # Porcentaje de datos para el conjunto de prueba\n",
    "stratified_split = StratifiedShuffleSplit(n_splits=1, test_size=test_size, random_state=42)\n",
    "train_idx, test_idx = next(stratified_split.split(np.arange(len(targets)), targets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creación de samplers para los DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "test_sampler = SubsetRandomSampler(test_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creación de los DataLoaders con los samplers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset, batch_size=64, sampler=train_sampler)\n",
    "test_loader = DataLoader(dataset, batch_size=64, sampler=test_sampler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inicialización y entrenamiento del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training CNN model...\n",
      "Epoch 1/40, Loss: 2.076885842388461\n",
      "Epoch 2/40, Loss: 1.7120356641200758\n",
      "Epoch 3/40, Loss: 1.5837268177766977\n",
      "Epoch 4/40, Loss: 1.5176324511166686\n",
      "Epoch 5/40, Loss: 1.4618888352968678\n",
      "Epoch 6/40, Loss: 1.428621694908379\n",
      "Epoch 7/40, Loss: 1.3914653581121694\n",
      "Epoch 8/40, Loss: 1.3564483608518327\n",
      "Epoch 9/40, Loss: 1.3218818703053161\n",
      "Epoch 10/40, Loss: 1.2938289201777915\n",
      "Epoch 11/40, Loss: 1.249253613608224\n",
      "Epoch 12/40, Loss: 1.2292819563646493\n",
      "Epoch 13/40, Loss: 1.193581369352637\n",
      "Epoch 14/40, Loss: 1.1601481741259556\n",
      "Epoch 15/40, Loss: 1.1334828390097766\n",
      "Epoch 16/40, Loss: 1.1029082259035998\n",
      "Epoch 17/40, Loss: 1.0676490520098194\n",
      "Epoch 18/40, Loss: 1.0291181236320401\n",
      "Epoch 19/40, Loss: 0.9882279323494952\n",
      "Epoch 20/40, Loss: 0.952714875247908\n",
      "Epoch 21/40, Loss: 0.9454283133056594\n",
      "Epoch 22/40, Loss: 0.8861193486622402\n",
      "Epoch 23/40, Loss: 0.8792990901455375\n",
      "Epoch 24/40, Loss: 0.822005171583306\n",
      "Epoch 25/40, Loss: 0.8133489937515732\n",
      "Epoch 26/40, Loss: 0.767491344709574\n",
      "Epoch 27/40, Loss: 0.7408209537497218\n",
      "Epoch 28/40, Loss: 0.7321416239560761\n",
      "Epoch 29/40, Loss: 0.6979190727568562\n",
      "Epoch 30/40, Loss: 0.6649367155865853\n",
      "Epoch 31/40, Loss: 0.6551169671627305\n",
      "Epoch 32/40, Loss: 0.6280163132255863\n",
      "Epoch 33/40, Loss: 0.6121952443019204\n",
      "Epoch 34/40, Loss: 0.5888416815248335\n",
      "Epoch 35/40, Loss: 0.5603197870417411\n",
      "Epoch 36/40, Loss: 0.5470681020191738\n",
      "Epoch 37/40, Loss: 0.5373749727417964\n",
      "Epoch 38/40, Loss: 0.5219886678716411\n",
      "Epoch 39/40, Loss: 0.4984791452840248\n",
      "Epoch 40/40, Loss: 0.48556456173428836\n"
     ]
    }
   ],
   "source": [
    "model = AffectNetCNN(num_classes=num_classes, lr=0.0001)\n",
    "\n",
    "print('Training CNN model...')\n",
    "model.train_(train_loader, epochs=40, class_weights=class_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluación del modelo en el conjunto de prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.60\n"
     ]
    }
   ],
   "source": [
    "accuracy = model.test_(test_loader)\n",
    "print(f'Test Accuracy: {accuracy:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Guardado del modelo entrenado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'model.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
